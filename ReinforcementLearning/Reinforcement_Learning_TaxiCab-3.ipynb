{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# OpenAi and Reinforcement Learning\n",
    "\n",
    "In this notebook we will:\n",
    "\n",
    "1. Introduce programming with the [OpenAi gym environment](https://gym.openai.com)\n",
    "\n",
    "2. Introduce Reinforcement Learning with the Q-Learning Algorithm\n",
    "\n",
    "With this aim, we first need to install the latest version of gym and in my case, also install the cmake gym atari interface. This can be done by running the following code cell. \n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: cmake in c:\\anaconda\\lib\\site-packages (3.22.1)\n",
      "Requirement already satisfied: gym[atari] in c:\\anaconda\\lib\\site-packages (0.21.0)\n",
      "Requirement already satisfied: scipy in c:\\anaconda\\lib\\site-packages (1.5.2)\n",
      "Requirement already satisfied: numpy>=1.18.0 in c:\\anaconda\\lib\\site-packages (from gym[atari]) (1.19.2)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\anaconda\\lib\\site-packages (from gym[atari]) (1.6.0)\n",
      "Requirement already satisfied: ale-py~=0.7.1; extra == \"atari\" in c:\\anaconda\\lib\\site-packages (from gym[atari]) (0.7.3)\n",
      "Requirement already satisfied: importlib-resources in c:\\anaconda\\lib\\site-packages (from ale-py~=0.7.1; extra == \"atari\"->gym[atari]) (5.4.0)\n",
      "Requirement already satisfied: zipp>=3.1.0; python_version < \"3.10\" in c:\\anaconda\\lib\\site-packages (from importlib-resources->ale-py~=0.7.1; extra == \"atari\"->gym[atari]) (3.4.0)\n",
      "Requirement already satisfied: gym in c:\\anaconda\\lib\\site-packages (0.21.0)\n",
      "Requirement already satisfied: numpy>=1.18.0 in c:\\anaconda\\lib\\site-packages (from gym) (1.19.2)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\anaconda\\lib\\site-packages (from gym) (1.6.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install cmake gym[atari] scipy\n",
    "!pip install gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "\n",
    "## The Gym Environment\n",
    "\n",
    "The core gym interface is ``env``, which is the unified environment interface. The following list shows the basic ``env`` methods:\n",
    "\n",
    "1. ``env.reset()``: Resets the environment and returns a random initial state.\n",
    "\n",
    "2. ``env.step(action)``: Step the environment by one timestep. Returns\n",
    "\n",
    " - observation (object): This is the current state of the environment and is an environment-specific object representing your observation of the environment. For example, pixel data from a camera, joint angles and joint velocities of a robot, or the board state in a board game.\n",
    "\n",
    " - reward (float): amount of reward achieved by the previous action. The scale varies between environments, but the goal is always to increase your total reward.\n",
    "\n",
    " - done (boolean): whether it’s time to reset the environment again. Most (but not all) tasks are divided up into well-defined episodes, and done being True indicates the episode has terminated. (For example, perhaps the pole tipped too far, or you lost your last life.)\n",
    "\n",
    " - info (dict): diagnostic information useful for debugging. It can sometimes be useful for learning (for example, it might contain the raw probabilities behind the environment’s last state change). However, official evaluations of your agent are not allowed to use this for learning.\n",
    "\n",
    "7. ``env.render()``: Renders one frame of the environment (helpful in visualizing the environment)\n",
    "\n",
    "\n",
    "\n",
    "We can best illustrate these notions with an example such as the Taxi-v3 environment. \n",
    "\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :G|\n",
      "| :\u001b[43m \u001b[0m| : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import the gym environment \n",
    "import gym\n",
    "\n",
    "# Instantiate the taxi environment \n",
    "env = gym.make(\"Taxi-v3\").env\n",
    "\n",
    "# Reset the environment \n",
    "env.reset()\n",
    "\n",
    "# Show the current frame of the environment \n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## The Taxi-v3 environment\n",
    "\n",
    "There are 4 locations (labeled by different letters), and our job is to pick up the passenger at one location and drop him off at another. We receive +20 points for a successful drop-off and lose 1 point for every time-step it takes. There is also a 10 point penalty for illegal pick-up and drop-off actions\n",
    "\n",
    "* Observations: There are 500 discrete states since there are 25 taxi positions, 5 possible locations of the passenger (including the case when the passenger is in the taxi), and 4 destination locations. \n",
    "   \n",
    "* Passenger locations:\n",
    " - 0: R(ed)\n",
    " - 1: G(reen)\n",
    " - 2: Y(ellow)\n",
    " - 3: B(lue)\n",
    " - 4: in taxi\n",
    "  \n",
    "* Destinations:\n",
    " - 0: R(ed)\n",
    " - 1: G(reen)\n",
    " - 2: Y(ellow)\n",
    " - 3: B(lue)\n",
    "   \n",
    "* Actions: There are 6 discrete deterministic actions:\n",
    " - 0: move south\n",
    " - 1: move north\n",
    " - 2: move east\n",
    " - 3: move west\n",
    " - 4: pickup passenger\n",
    " - 5: drop off passenger\n",
    " \n",
    "* Rewards: There is a default per-step reward of -1, except for delivering the passenger, which is +20, or executing \"pickup\" and \"drop-off\" actions illegally, which is -10.\n",
    "\n",
    "* Rendering:\n",
    " - blue: passenger\n",
    " - magenta: destination\n",
    " - yellow: empty taxi\n",
    " - green: full taxi\n",
    " - other letters (R, G, Y and B): locations for passengers and destinations\n",
    "   \n",
    "* state space is represented by: (taxi_row, taxi_col, passenger_location, destination)\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m:\u001b[43m \u001b[0m| : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "action: 1 \n",
      "\n",
      "observation: 22\n",
      "\n",
      "reward: -1\n",
      "\n",
      "done: False \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Choose a random action\n",
    "action = np.random.randint(6)\n",
    "\n",
    "# Take a step\n",
    "observation, reward, done, info = env.step(action)\n",
    "\n",
    "# Show the current frame of the environment \n",
    "env.render()\n",
    "\n",
    "print(f\"action: {action} \\n\")\n",
    "\n",
    "print(f\"observation: {observation}\\n\")\n",
    "\n",
    "print(f\"reward: {reward}\\n\")\n",
    "\n",
    "print(f\"done: {done} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m:\u001b[43m \u001b[0m| : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (East)\n",
      "action: 2 \n",
      "\n",
      "observation: 22\n",
      "\n",
      "reward: -1\n",
      "\n",
      "done: False \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Choose a random action\n",
    "action = np.random.randint(6)\n",
    "\n",
    "# Take a step\n",
    "observation, reward, done, info = env.step(action)\n",
    "\n",
    "# Show the current frame of the environment \n",
    "env.render()\n",
    "\n",
    "print(f\"action: {action} \\n\")\n",
    "\n",
    "print(f\"observation: {observation}\\n\")\n",
    "\n",
    "print(f\"reward: {reward}\\n\")\n",
    "\n",
    "print(f\"done: {done} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :G|\n",
      "| :\u001b[43m \u001b[0m| : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (South)\n",
      "action: 0 \n",
      "\n",
      "observation: 122\n",
      "\n",
      "reward: -1\n",
      "\n",
      "done: False \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Choose a random action\n",
    "action = np.random.randint(6)\n",
    "\n",
    "# Take a step\n",
    "observation, reward, done, info = env.step(action)\n",
    "\n",
    "# Show the current frame of the environment \n",
    "env.render()\n",
    "\n",
    "print(f\"action: {action} \\n\")\n",
    "\n",
    "print(f\"observation: {observation}\\n\")\n",
    "\n",
    "print(f\"reward: {reward}\\n\")\n",
    "\n",
    "print(f\"done: {done} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "We can also manually set the state of the current ``env`` by first using the ``env.encode`` method and then setting the current state (``env.s``) to be this new state. The following code cell illustrates this option. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State: 48\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: |\u001b[43m \u001b[0m: :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (South)\n"
     ]
    }
   ],
   "source": [
    "# Use the env.encode method \n",
    "state = env.encode(0, 2, 2, 0) # (taxi row, taxi column, passenger index, destination index)\n",
    "print(f\"State: {state}\")\n",
    "\n",
    "# Set the current state of the environmnet \n",
    "env.s = state\n",
    "\n",
    "# Show the current frame of the environment \n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timesteps taken: 150 \n",
      "\n",
      "Penalties incurred: 46 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "\n",
    "epochs = 0\n",
    "penalties = 0\n",
    "reward = 0\n",
    "max_iter = 150\n",
    "\n",
    "frames = [] # for animation\n",
    "\n",
    "done = False\n",
    "\n",
    "while not done and epochs < max_iter:\n",
    "    action = env.action_space.sample()\n",
    "    state, reward, done, info = env.step(action)\n",
    "\n",
    "    if reward == -10:\n",
    "        penalties += 1\n",
    "    \n",
    "    # Put each rendered frame into dict for animation\n",
    "    frames.append({\n",
    "        'frame': env.render(mode='ansi'),\n",
    "        'state': state,\n",
    "        'action': action,\n",
    "        'reward': reward\n",
    "        }\n",
    "    )\n",
    "\n",
    "    epochs += 1\n",
    "    \n",
    "    \n",
    "print(f\"Timesteps taken: {epochs} \\n\")\n",
    "print(f\"Penalties incurred: {penalties} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|R: |\u001b[42m_\u001b[0m: :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (Pickup)\n",
      "\n",
      "Timestep: 150\n",
      "State: 59\n",
      "Action: 4\n",
      "Reward: -10\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "from time import sleep\n",
    "\n",
    "def print_frames(frames):\n",
    "    for i, frame in enumerate(frames):\n",
    "        clear_output(wait=True)\n",
    "        print(frame['frame'])\n",
    "        #print(frame['frame'].getvalue())\n",
    "        print(f\"Timestep: {i + 1}\")\n",
    "        print(f\"State: {frame['state']}\")\n",
    "        print(f\"Action: {frame['action']}\")\n",
    "        print(f\"Reward: {frame['reward']}\")\n",
    "        sleep(.1)\n",
    "        \n",
    "\n",
    "print_frames(frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: [(1.0, 448, -1, False)],\n",
       " 1: [(1.0, 348, -1, False)],\n",
       " 2: [(1.0, 448, -1, False)],\n",
       " 3: [(1.0, 428, -1, False)],\n",
       " 4: [(1.0, 448, -10, False)],\n",
       " 5: [(1.0, 448, -10, False)]}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.P[448]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Introduction to Reinforcement Learning  \n",
    "\n",
    "Reinforcement learning is learning what to do; how to map situations to actions as to maximize a numerical reward signal. The learner, or agent, is not told which actions to take, but to instead discover which actions yield the most reward by trying them. This type of model can be thought of a specific instance of Markov decision processes (MDP's). The learner and action maker is called the **agent**. The thing that the agent interacts with is called the **environment**, which can be thought of as everything outside the agent. \n",
    "\n",
    "The agent and environment interact in a looping process, where the agent observes some portion of the environment and takes an action, after which, the environment responds and presents a new situation to the agent. More specifically, the agent and environment  interact at each of a sequence of discrete time steps $t = 0, 1, \\dots, T$, where $T$ is the **terminal state**. At each time step $t$, the agent recieves some representation of the environment's **state**, $S_t \\in \\mathcal{S}$, and on that basis selects an **action**, $A_t \\in \\mathcal{A}$. Here, $\\mathcal{S}$ is the set of all possible states and $\\mathcal{A}$ is the set of all possible/valid actions. One time step later, and in part as a consequence of action $A_t$, the agents recieves a numerical **reward**, $R_{t+1} \\in \\mathbb{R}$, and finds itself in a new state, $S_{t+1}\\in \\mathcal{S}$. The MDP and agent together give rise to a sequence, or **trajectory** typically denoted by $\\tau$:\n",
    "\n",
    "$$\n",
    "\\tau = S_0, A_0, R_1, S_1, A_1, R_2, S_2, A_2, R_3, \\dots \n",
    "$$\n",
    "\n",
    "\n",
    "The goal of the agent is to maximize its rewards over a given trajectory starting from state $S_t$. This is called the **return** and is given with the following equation:\n",
    "\n",
    "$$\n",
    "G_{t} = R_{t+1} + \\gamma R_{t+2} + \\gamma^{2} R_{t+3} \\dots, \n",
    "$$\n",
    "\n",
    "where $\\gamma \\in [0, 1]$ is the **discount rate**. The discount rate determines the present value of the future rewards. This formula can be written recursively as:\n",
    "\n",
    "$$\n",
    "G_{t} = R_{t+1} + \\gamma G_{t+1}. \n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "## Policies and the Q-Function\n",
    "\n",
    "A **policy** is a mapping from states to probabilities of selecting each possible action. \n",
    "\n",
    "$$\n",
    "q_{\\pi}(s, a) = \\mathbb{E}_{\\pi}\\Big[ G_t | S_t = s, A_t = a\\Big] = \\mathbb{E}_{\\pi}\\Big[ G_t + \\gamma G_{t+1}| S_t = s, A_t = a\\Big]\n",
    "$$\n",
    "\n",
    "Let $Q(S_t, A_t)$ denote the current q-value of the state action pair $(S_t, A_t)$. Through experience, the agent can learn how well our current estimate is (just like we compare predicted labels to true labels in supervised learning). The agent can then update the value of $Q(S_t, A_t)$ after experiencing its future rewards. The following update rule illustrates this updating:\n",
    "\n",
    "$$\n",
    "Q(S, A) \\leftarrow Q(S, A) + \\alpha \\Big[R + \\gamma \\max_{a}Q(S', a) - Q(S, A) \\Big]\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the Q-table is: (500, 6) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "# Initialize Q-values as a Q-table of 0's \n",
    "q_table = np.zeros([env.observation_space.n, env.action_space.n])\n",
    "\n",
    "print(f\"The shape of the Q-table is: {q_table.shape} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 10000\n",
      "Training finished.\n",
      "\n",
      "State: 44\n",
      "[-2.43904593 -2.42196411 -2.41837066 -2.42755423 -8.0930273  -5.10457054]\n",
      "0\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: |\u001b[43m \u001b[0m: :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (Dropoff)\n",
      "Wall time: 6.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\"\"\"Training the agent\"\"\"\n",
    "\n",
    "import random\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# Hyperparameters\n",
    "alpha = 0.1\n",
    "gamma = 0.6\n",
    "epsilon = 0.1\n",
    "\n",
    "# For plotting metrics\n",
    "all_epochs = []\n",
    "all_penalties = []\n",
    "frames = []\n",
    "for i in range(1, 10_001):\n",
    "    state = env.reset()\n",
    "\n",
    "    epochs, penalties, reward, = 0, 0, 0\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        if random.uniform(0, 1) < epsilon:\n",
    "            action = env.action_space.sample() # Explore action space\n",
    "        else:\n",
    "            action = np.argmax(q_table[state]) # Exploit learned values\n",
    "\n",
    "        next_state, reward, done, info = env.step(action) \n",
    "        \n",
    "        old_value = q_table[state, action]\n",
    "        next_max = np.max(q_table[next_state])\n",
    "        \n",
    "        # Q-table update Rule \n",
    "        new_value = (1 - alpha) * old_value + alpha * (reward + gamma * next_max)\n",
    "        \n",
    "        q_table[state, action] = new_value\n",
    "\n",
    "        if reward == -10:\n",
    "            penalties += 1\n",
    "\n",
    "        \n",
    "        state = next_state\n",
    "        epochs += 1\n",
    "        \n",
    "    \n",
    "        \n",
    "    if i % 100 == 0:\n",
    "        clear_output(wait=True)\n",
    "        print(f\"Episode: {i}\")\n",
    "\n",
    "print(\"Training finished.\\n\")\n",
    "\n",
    "\n",
    " \n",
    "state = env.encode(0, 2, 1, 0) # (taxi row, taxi column, passenger index, destination index)\n",
    "print(\"State:\", state)\n",
    "print(q_table[state])\n",
    "env.s = state\n",
    "print(np.argmax(state))\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State: 48\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: |\u001b[43m \u001b[0m: :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (Dropoff)\n"
     ]
    }
   ],
   "source": [
    "state = env.encode(0, 2, 2, 0) # (taxi row, taxi column, passenger index, destination index)\n",
    "print(\"State:\", state)\n",
    "\n",
    "env.s = state\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.41841907, -2.42216034, -2.42126235, -2.42155608, -5.10984586,\n",
       "       -6.31735057])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_table[48]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results after 2 episodes:\n",
      "Average timesteps per episode: 14.0\n",
      "Average penalties per episode: 0.0\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Evaluate agent's performance after Q-learning\"\"\"\n",
    "\n",
    "total_epochs, total_penalties = 0, 0\n",
    "episodes = 2\n",
    "frames = []\n",
    "for _ in range(episodes):\n",
    "    state = env.reset()\n",
    "    epochs, penalties, reward = 0, 0, 0\n",
    "    \n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        action = np.argmax(q_table[state])\n",
    "        state, reward, done, info = env.step(action)\n",
    "\n",
    "        if reward == -10:\n",
    "            penalties += 1\n",
    "        # Put each rendered frame into dict for animation\n",
    "        frames.append({\n",
    "            'frame': env.render(mode='ansi'),\n",
    "            'state': state,\n",
    "            'action': action,\n",
    "            'reward': reward\n",
    "                            }\n",
    "            )\n",
    "        epochs += 1\n",
    "\n",
    "    total_penalties += penalties\n",
    "    total_epochs += epochs\n",
    "\n",
    "print(f\"Results after {episodes} episodes:\")\n",
    "print(f\"Average timesteps per episode: {total_epochs / episodes}\")\n",
    "print(f\"Average penalties per episode: {total_penalties / episodes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35m\u001b[34;1m\u001b[43mB\u001b[0m\u001b[0m\u001b[0m: |\n",
      "+---------+\n",
      "  (Dropoff)\n",
      "\n",
      "Timestep: 28\n",
      "State: 475\n",
      "Action: 5\n",
      "Reward: 20\n"
     ]
    }
   ],
   "source": [
    "print_frames(frames)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
